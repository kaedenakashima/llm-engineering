{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1cbba1-8059-49a3-92e2-d5d276144efb",
   "metadata": {},
   "source": [
    "# Langchain Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ead8365-ba14-4e41-947a-ede28deed017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting callbacks\n",
      "  Downloading callbacks-0.3.0.tar.gz (9.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25done\n",
      "\u001b[?25hCollecting langchain_pinecone\n",
      "  Downloading langchain_pinecone-0.2.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain_pinecone) (0.3.40)\n",
      "Collecting pinecone<6.0.0,>=5.4.0 (from langchain_pinecone)\n",
      "  Using cached pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aiohttp<3.11,>=3.10 (from langchain_pinecone)\n",
      "  Downloading aiohttp-3.10.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain_pinecone) (1.26.4)\n",
      "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
      "  Downloading langchain_tests-0.3.17-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.18.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.10.6)\n",
      "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain_pinecone)\n",
      "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pytest<9,>=7 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
      "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
      "  Using cached pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2025.1.31)\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone<6.0.0,>=5.4.0->langchain_pinecone)\n",
      "  Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<6.0.0,>=5.4.0->langchain_pinecone)\n",
      "  Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2.3.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.27.2)\n",
      "Collecting iniconfig (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
      "  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone<6.0.0,>=5.4.0->langchain_pinecone) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp<3.11,>=3.10->langchain_pinecone) (0.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from anyio->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.3.1)\n",
      "Downloading langchain_pinecone-0.2.3-py3-none-any.whl (11 kB)\n",
      "Downloading aiohttp-3.10.11-cp311-cp311-macosx_11_0_arm64.whl (392 kB)\n",
      "Downloading langchain_tests-0.3.17-py3-none-any.whl (39 kB)\n",
      "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
      "Using cached pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
      "Using cached pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
      "Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
      "Using cached pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
      "Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Building wheels for collected packages: callbacks\n",
      "  Building wheel for callbacks (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for callbacks: filename=callbacks-0.3.0-py3-none-any.whl size=5689 sha256=90ac19b119e525560ac371c8bef139eb111b3dca7dd4a9eee2fb7043ef2deccc\n",
      "  Stored in directory: /Users/katie/Library/Caches/pip/wheels/b9/d3/16/a3f93d3628ff8ca6025ef62cea56ca0e49d0be67cd70923e4f\n",
      "Successfully built callbacks\n",
      "Installing collected packages: callbacks, pluggy, pinecone-plugin-interface, iniconfig, pytest, pinecone-plugin-inference, aiohttp, syrupy, pytest-socket, pytest-asyncio, pinecone, langchain-core, langchain-tests, langchain_pinecone\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.13\n",
      "    Uninstalling aiohttp-3.11.13:\n",
      "      Successfully uninstalled aiohttp-3.11.13\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.40\n",
      "    Uninstalling langchain-core-0.3.40:\n",
      "      Successfully uninstalled langchain-core-0.3.40\n",
      "Successfully installed aiohttp-3.10.11 callbacks-0.3.0 iniconfig-2.1.0 langchain-core-0.3.49 langchain-tests-0.3.17 langchain_pinecone-0.2.3 pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7 pluggy-1.5.0 pytest-8.3.5 pytest-asyncio-0.26.0 pytest-socket-0.7.0 syrupy-4.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install callbacks langchain_pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517edf6-3af2-45a2-b34c-c30648f9b361",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d50daa8-36da-459a-bb9a-42620108d808",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AgentCallbackhandler' from 'callbacks' (/opt/anaconda3/envs/llms/lib/python3.11/site-packages/callbacks/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseCallbackHandler\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMResult\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentCallbackhandler\n\u001b[1;32m     12\u001b[0m load_dotenv()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAgentCallbackhandler\u001b[39;00m(BaseCallbackHandler):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AgentCallbackhandler' from 'callbacks' (/opt/anaconda3/envs/llms/lib/python3.11/site-packages/callbacks/__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import Union, List\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.tools import Tool, tool\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from callbacks import AgentCallbackhandler\n",
    "load_dotenv()\n",
    "\n",
    "class AgentCallbackhandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "            self, serialized:Dict[str,Any],prompts:List[str],**kwargs:Any\n",
    "    )->Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "        print(f'**Prmpt to LLM was:**\\n{prompts[0]}')\n",
    "        print('***')\n",
    "    def on_llm_end(self, response:LLMResult, **kwargs:Any)->Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        print('***LLM Response:***\\n{response.generations[0][0].text}')\n",
    "        print('****')\n",
    "    \n",
    "@tool\n",
    "def get_text_length(text:str)->int:\n",
    "    \"\"\"Returns the length of a text by characters\"\"\"\n",
    "    text=text.strip(\"'\\n\").strip(\n",
    "        '\"'\n",
    "    )\n",
    "    return len(text)\n",
    "\n",
    "def find_tool_by_name(tools:List[Tool],tool_name:str)->Tool:\n",
    "    for tool in tools:\n",
    "        if tool.name == tool.name:\n",
    "            return tool\n",
    "    raise ValueError(f'Tool with name {tool_name} not found')\n",
    "\n",
    "tools = [get_text_length]\n",
    "template=\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template).partial(\n",
    "    tools=render_text_description(tools), \n",
    "    tool_names=', '.join([t.name for t in tools])\n",
    ")\n",
    "llm=ChatOpenAI(temperature=0,stop='Observation', callbacks=[AgentCallbackhandler()])\n",
    "intermediate_steps=[]\n",
    "agent = {'input':lambda x:x['input']}| prompt | llm | ReActSingleInputOutputParser()\n",
    "agent_step:Union[AgentAction,AgentFinish] = agent.invoke({\n",
    "    \"input\":\"What is the length in characters of the text DOG?\",\n",
    "    \"agent_scrachpad\":intermediate_steps\n",
    "})\n",
    "print(agent_step)\n",
    "if isinstance(agent_step, AgentAction):\n",
    "    tool_name=agent_step.tool\n",
    "    tool_to_use=find_tool_by_name(tools,tool_name)\n",
    "    tool_input=agent_step.tool_input\n",
    "    observation=tool_to_use.func(str(tool_input))\n",
    "    print(f\"{observation=}\")\n",
    "    intermediate_steps.append((agent_step, str(observation)))\n",
    "\n",
    "if isinstance(agent_step, AgentFinish):\n",
    "    print(agent_step.return_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae9352-43d4-4f85-8d1c-f373c63b328f",
   "metadata": {},
   "source": [
    "# Agent LangChain + Tavily to find account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b17c03-1921-463e-b2c9-37b446afc6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tool\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_ollama'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    create_react_agent,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "def lookup(name: str) -> str:\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name='gpt-4o-mini',\n",
    "        openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    )\n",
    "    # llm = ChatOllama(model='llama3')\n",
    "    template = \"\"\"given the name {name_of_person} I want you to find a link to their Twitter profile page, and extract from it their username\n",
    "    In Your Final answer only the person:s username which is extracted from: https://x.com/USERNAME\"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=template, input_variables=[name_of_person']\n",
    "    )\n",
    "    tools_for_agent = [\n",
    "        Tool(\n",
    "            name='Crawl Google 4 Twitter profile page',\n",
    "            func=get_profile_url_tavily,\n",
    "            description='useful for when you need get the Twitter Page URL',\n",
    "        )\n",
    "    ]\n",
    "    react_prompt = hub.pull('hwchase17/react')\n",
    "    agent = create_react_agent(llm=llm, tools=tools_for_agent, prompt=react_prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools_for_agent, verbose=True)\n",
    "    result = agent_executor.invoke(\n",
    "        input={'input': prompt_template.format_prompt(name_of_person=name)}\n",
    "    )\n",
    "    return result['output']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5042ef7-055a-4c40-814f-b4652cf6d9c3",
   "metadata": {},
   "source": [
    "# QA LangChain vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949f57ee-af42-429f-bffe-cceda471e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone is a fully managed cloud-based vector database designed for fast and scalable retrieval of similar data points based on their vector representations, supporting large-scale ML applications with high query throughput and low latency search. Pinecone is secure, user-friendly, and provides real-time updates for efficient database maintenance.\n",
      "Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "load_dotenv()\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "embeddings=OpenAIEmbeddings()\n",
    "llm = ChatOpenAI()\n",
    "query = 'What is Pinecone in machine learning?'\n",
    "chain = PromptTemplate.from_template(template=query) | llm\n",
    "vectorstore=PineconeVectorStore(\n",
    "    index_name=os.environ['INDEX_NAME'],embedding=embeddings\n",
    ")\n",
    "retrieval_qa_chat_prompt=hub.pull('langchain-ai/retrieval-qa-chat')\n",
    "combine_docs_chain=create_stuff_documents_chain(llm,retrieval_qa_chat_prompt)\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever=vectorstore.as_retriever(), combine_docs_chain=combine_docs_chain\n",
    ")\n",
    "result = retrieval_chain.invoke(input={'input':query})\n",
    "\n",
    "template=\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't now, don't try to make up an answer.\n",
    "Use three sentenses maximum and keep the answer as concse as possible.\n",
    "Always say 'thanks for asking!' at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answer: \n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "rag_chain = (\n",
    "    {'context':vectorstore.as_retriever()| format_docs, 'question':RunnablePassthrough()}\n",
    "    |custom_rag_prompt\n",
    "    |llm\n",
    ")\n",
    "res=rag_chain.invoke(query)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da2665-9df2-425f-b36c-a6285597b01c",
   "metadata": {},
   "source": [
    "# QA LangChain PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1133b-f32f-4640-8360-ec49e3149cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "load_dotenv()\n",
    "# os.environ['OPEN_AI_KEY']\n",
    "from langchain.docstore.document import Document\n",
    "loader = PyPDFLoader('/Users/katie/Documents/development/python/llm/vectorstor-in-memory/ocred_doc1.pdf')\n",
    "documents=loader.load()\n",
    "text_splitter=CharacterTextSplitter(chunk_size=1000,chunk_overlap=30,separator='\\n')\n",
    "docs=text_splitter.split_documents(documents=documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "vectorstore.save_local('/Users/katie/Documents/development/python/llm/vectorstor-in-memory/faiss_index_react')\n",
    "new_vectorstore=FAISS.load_local('/Users/katie/Documents/development/python/llm/vectorstor-in-memory/faiss_index_react', embeddings, allow_dangerous_deserialization=True)\n",
    "retrieval_qa_chat_prompt = hub.pull('langchain-ai/retrieval-qa-chat')\n",
    "combine_docs_chain = create_stuff_documents_chain(OpenAI(), retrieval_qa_chat_prompt)\n",
    "retrieval_chain= create_retrieval_chain(new_vectorstore.as_retriever(),combine_docs_chain)\n",
    "res = retrieval_chain.invoke({'input':'Give me the gist of ReAct in 3 sentences'})\n",
    "print(res['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114b8c8-31e5-4bb1-839e-e53e585a175c",
   "metadata": {},
   "source": [
    "# Practice Langchain PromptTemplate 1\n",
    "\n",
    "**input_variables:**\n",
    "When partial_variables are not included, 1st variable have to be question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72dc56d2-cadc-42bd-b7c9-e160b73f5e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The founder of Apple is Steve Jobs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')   \n",
    "openai = OpenAI()\n",
    "template: str = \"\"\"/\n",
    "You are an AI agent. give responses to the following/ \n",
    "question: {question}. Use simple words and short sentence.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt_formatted_str: str = prompt.format(question=\"Who is founder of Apple?\")\n",
    "print(openai.predict(prompt_formatted_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e370a4-24b6-42db-93c7-2f4fb17786c5",
   "metadata": {},
   "source": [
    "# Summarise Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b74b1a1-8fd8-4b00-b23a-a3c377a5b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "The article discusses the top 5 paid subscriptions that the author, a Staff Software Engineer, uses in 2025 to improve their performance and productivity. They highlight the benefits of each tool and why they find them worth the investment.\n",
      "\n",
      "5 Important Things:\n",
      "\n",
      "1. The author values the tools they use and believes that investing in them is important for their performance as a software engineer.\n",
      "2. They use Cursor, an AI programming tool, for code completion and code-aware chat.\n",
      "3. The author finds Cursor to be more reliable and useful than GitHub Copilot.\n",
      "4. They also use Kagi, a search engine, which provides consistently accurate results and personalized search options.\n",
      "5. The author emphasizes the importance of personalization and effectiveness when it comes to choosing tools for their work. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')   \n",
    "openai = OpenAI()\n",
    "template: str = \"\"\"\n",
    "    Given the information about article {question} I want you to create:\n",
    "    1. a short summary\n",
    "    2. list up 5 important things\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt_formatted_str: str = prompt.format(\n",
    "    question= 'The 5 paid subscriptions I actually use in 2025 as a Staff Software Engineer Tools I use that are cheaper than Netflix Jacob Bennett Level Up Coding Jacob Bennett · Follow Published in Level Up Coding · 5 min read · Jan 8, 2025 11.7K 291 I care a lot about the tools I use. Especially when they aren’t free. Here’s what I’m paying for in 2025 to improve my performance and productivity as a software engineer. Please note: None of the links in this article are affiliate links. Cursor: the best AI programming tool Cursor feels like a bargain for the level-up it provides me. I’m an average programmer with a strong preference for back-end projects, but Cursor lets me work across the full stack with the clarity and quality of a much better engineer. A chat I had with Cursor regarding a copy-pasted code block Along with code completion, Cursor has the best chat of any code-aware chat tool I’ve used. Usually I’ll just highlight a code block that I’m looking at, press CMD + L, ask a question in natural language, and Cursor load all of the relevant context from the current project and external sources (e.g. package docs) and give me an immediately-useful answer. Learn more about Cursor → What about GitHub Copilot? I used Copilot for 8 months in 2024. It was great! (I even included it on my 2024 list.) But Cursor blows it out of the water. Copilot was the most accurate when I was writing generic/boilerplate code (e.g. API endpoints) or test cases with lots of repeated code blocks. Anything more complex (especially logic spanning multiple files and modules) and Copilot was unreliable. Cursor feels like an actual context-aware pair programmer with a decent understanding of the project I’m looking at. Kagi: a better search engine than Google I measure the effectiveness of searches by how long it takes me to find what I was actually looking for. By that measure, Google has been steadily getting worse. When I search for something on Kagi, the correct result is in the first 2 links 95% of the time. It’s in the top 5 links 99% of the time. That just doesn’t happen with Google, Bing, etc. The consistently great results page is further boosted by the search personalization… The 5 paid subscriptions I actually use in 2025 as a Staff Software Engineer Tools I use that are cheaper than Netflix Jacob Bennett Level Up Coding Jacob Bennett · Follow Published in Level Up Coding · 5 min read · Jan 8, 2025 11.7K 291 I care a lot about the tools I use. Especially when they aren’t free. Here’s what I’m paying for in 2025 to improve my performance and productivity as a software engineer. Please note: None of the links in this article are affiliate links. Cursor: the best AI programming tool Cursor feels like a bargain for the level-up it provides me. I’m an average programmer with a strong preference for back-end projects, but Cursor lets me work across the full stack with the clarity and quality of a much better engineer. A chat I had with Cursor regarding a copy-pasted code block Along with code completion, Cursor has the best chat of any code-aware chat tool I’ve used. Usually I’ll just highlight a code block that I’m looking at, press CMD + L, ask a question in natural language, and Cursor load all of the relevant context from the current project and external sources (e.g. package docs) and give me an immediately-useful answer. Learn more about Cursor → What about GitHub Copilot? I used Copilot for 8 months in 2024. It was great! (I even included it on my 2024 list.) But Cursor blows it out of the water. Copilot was the most accurate when I was writing generic/boilerplate code (e.g. API endpoints) or test cases with lots of repeated code blocks. Anything more complex (especially logic spanning multiple files and modules) and Copilot was unreliable. Cursor feels like an actual context-aware pair programmer with a decent understanding of the project I’m looking at. Kagi: a better search engine than Google I measure the effectiveness of searches by how long it takes me to find what I was actually looking for. By that measure, Google has been steadily getting worse. When I search for something on Kagi, the correct result is in the first 2 links 95% of the time. It’s in the top 5 links 99% of the time. That just doesn’t happen with Google, Bing, etc. The consistently great results page is further boosted by the search personalization…'\n",
    ")\n",
    "print(openai.predict(prompt_formatted_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
