{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53be2eec-7f8c-43fa-af47-d901b63d7bb8",
   "metadata": {},
   "source": [
    "# Hugging Face Collection 1ðŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fccd242-ae73-4b4a-9f53-cce4f0725578",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Index\n",
    "1-1 Answer with set system and model\n",
    "\n",
    "1-2 Answer with set model\n",
    "\n",
    "1-3 Answer with set provider and model\n",
    "\n",
    "2-1 Conversation with system\n",
    "\n",
    "2-2 Conversations with set system and model\n",
    "\n",
    "3   Generate Code\n",
    "\n",
    "4   Language\n",
    "\n",
    "5   Measure Customer Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d487592-8df0-4b9b-bcd8-9f943a8a84cd",
   "metadata": {},
   "source": [
    "#### API Menu\n",
    "\n",
    "#### 1 hf.chat.completions.create \n",
    "#### item:\n",
    "    model= 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "    messages = [{'role': 'user', 'content': 'system settiing'}]\n",
    "#### return:\n",
    "    res.choices[0].message.content\n",
    "#### 2 hf.chat_completion\n",
    "#### item:\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    messages=[{'role': 'user', 'content':'system setting'}]\n",
    "    \n",
    "#### return:\n",
    "    res.choices[0].message.content\n",
    "\n",
    "#### 2 hf.text_generation\n",
    "#### hf setting:\n",
    "    provider='hf-inference'\n",
    "\tapi_key='hf+token'\n",
    "#### item:\n",
    "\tmodel='google/gemma-2-2b-it'\n",
    "\tinputs='Can you please let us know more details about your '\n",
    "\tprovider='hf-inference'\n",
    "#### return:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcea7c-aecb-4f10-9d28-96120212e7f2",
   "metadata": {},
   "source": [
    "# 1-1 Answer with set system and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63316f9b-07cd-4d3d-bb41-e36b25185e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "load_dotenv()\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "text='Hello. How are you doing today?'\n",
    "def system_and_script(x1):\n",
    "    x2='Please answer with short sentence.'\n",
    "    x2+=x1\n",
    "    return x2\n",
    "messages = [{'role': 'user', 'content':system_and_script(text)}]\n",
    "llm = InferenceClient('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "res = llm.chat_completion(messages, max_tokens=100)\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828aaca-178e-427b-85cc-f8352cf87026",
   "metadata": {},
   "source": [
    "# 1-2 Answer with set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068ec573-e606-4cda-b29d-1d233e65193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just an AI, I don't have emotions or feelings like humans do, so I don't have a good or bad day. I'm always \"on\" and ready to assist you with any questions or tasks you may have! It's great to chat with you, though. How about you? How's your day going?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "load_dotenv()\n",
    "hf = InferenceClient(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "def say(x):\n",
    "    messages = [{'role': 'user', 'content': x}]\n",
    "    llm = hf.chat_completion(messages, max_tokens=100)\n",
    "    return llm.choices[0].message.content\n",
    "print(say('Hello. How are you doing?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70394c-62a4-4bc9-9a5c-ad351f1b7e42",
   "metadata": {},
   "source": [
    "# 1-3 Answer with set provider and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc33891f-6c33-4eff-94c7-7f6dc2d183bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States is Washington, D.C. (short for District of Columbia).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "load_dotenv()\n",
    "hf = InferenceClient(provider='hf-inference', api_key=os.environ['HF_TOKEN'])\n",
    "# provider can be hf-inference, fireworks-ai, black-forest-labs,  hyperbolic, nebius, novita\n",
    "def say(x):\n",
    "    llm = hf.chat_completion(\n",
    "        model='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        messages=[{'role': 'user', 'content':x}],\n",
    "    )\n",
    "    return llm.choices[0].message.content\n",
    "print(say('Where is capital of united States?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697e441-124c-4645-908f-a31fc6248d01",
   "metadata": {},
   "source": [
    "# 2-1 Conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc8708c-bb38-44e2-b5b7-994cad7ef404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: What is LLM?\n",
      "B: LLM stands for Large Language Model.\n",
      "A: Thank you for the answer.\n",
      "B: LLMs are artificial intelligence models trained on vast amounts of text data to generate human-like language, such as text summaries, translations, and even entire articles.\n",
      "A: I see. Thank you!\n",
      "B: You're welcome!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "load_dotenv()\n",
    "hf = InferenceClient()\n",
    "SystemMsgs= ['']\n",
    "UserMsgs  = []\n",
    "def saySystem(input):\n",
    "    UserMsgs.append(input)\n",
    "    msg = [{'role': 'system', 'content': 'Please answer with short sentence.'}]\n",
    "    for x, y in zip(SystemMsgs, UserMsgs):\n",
    "        msg.append({'role': 'assistant', 'content': x})\n",
    "        msg.append({'role': 'user', 'content': y})\n",
    "    llm = hf.chat.completions.create(\n",
    "        model= 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        messages=msg\n",
    "    )\n",
    "    res = llm.choices[0].message.content\n",
    "    SystemMsgs.append(res)\n",
    "    return res\n",
    "print(f\"A: What is LLM?\")\n",
    "b_next = saySystem('What is LLM?')\n",
    "print(f\"B: {b_next}\")\n",
    "b_next = saySystem('Can you tell me more about it?')\n",
    "print(f\"A: Thank you for the answer.\")\n",
    "print(f\"B: {b_next}\")\n",
    "b_next = saySystem('I see. Thank you!')\n",
    "print(f\"A: I see. Thank you!\")\n",
    "print(f\"B: {b_next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e5d36-56fa-4d6c-af5d-c1684b204475",
   "metadata": {},
   "source": [
    "# 2-2 Conversations with set system and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e0b7f0-01c5-4dfb-8dc1-5008ef5b2967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:How can I help you today?\n",
      "\n",
      "B:Just answering questions!\n",
      "\n",
      "A:I'll keep my answers concise then!\n",
      "\n",
      "B:Smart thinking!\n",
      "\n",
      "A:Thanks!\n",
      "\n",
      "B:You're welcome!\n",
      "\n",
      "A:Done!\n",
      "\n",
      "B:All done!\n",
      "\n",
      "A:Exactly!\n",
      "\n",
      "B:Agreed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "load_dotenv()\n",
    "hf = InferenceClient()\n",
    "msgsA = ['Hi there']\n",
    "msgsB = ['Hi']\n",
    "def sayA():\n",
    "    msg = [{'role': 'system', 'content': 'Please answer with short sentence.'}]\n",
    "    for x, y in zip(msgsA, msgsB):\n",
    "        msg.append({'role': 'assistant', 'content': x})\n",
    "        msg.append({'role': 'user', 'content': y})\n",
    "    llm = hf.chat.completions.create(\n",
    "        model= 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        messages=msg\n",
    "    )\n",
    "    return llm.choices[0].message.content\n",
    "def sayB():\n",
    "    msg = [{'role': 'user', 'content': 'Please answer with short sentence.'}]\n",
    "    for x, y in zip(msgsA, msgsB):\n",
    "        msg.append({'role': 'user', 'content': x})\n",
    "        msg.append({'role': 'assistant', 'content': y})\n",
    "    msg.append({'role': 'user', 'content': msgsA[-1]})\n",
    "    llm = hf.chat.completions.create(\n",
    "        model= 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        messages=msg\n",
    "    )\n",
    "    return llm.choices[0].message.content\n",
    "\n",
    "for i in range(5):\n",
    "    a_next = sayA()\n",
    "    print(f\"A:{a_next}\\n\")\n",
    "    msgsA.append(a_next)\n",
    "    b_next = sayB()\n",
    "    print(f\"B:{b_next}\\n\")\n",
    "    msgsB.append(b_next)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92265445-100e-40cb-84db-c1c5165c55f9",
   "metadata": {},
   "source": [
    "# 2-3 Converation history and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58bfa0d9-1e8a-4de7-a2e9-be630df235d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: How are you today?\n",
      "tools ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content=\"I'm functioning properly, thanks!\", tool_calls=None), logprobs=None)\n",
      "B: omg!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "load_dotenv()\n",
    "hf = InferenceClient()\n",
    "SystemMsgs= ['']\n",
    "UserMsgs  = []\n",
    "def get_current_temperature(location: str):\n",
    "    \"\"\"\n",
    "    Gets the temperature at a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for\n",
    "    \"\"\"\n",
    "    return 22.0  \n",
    "\n",
    "tools = [get_current_temperature]\n",
    "def saySystem(input):\n",
    "    UserMsgs.append(input)\n",
    "    msg = [{'role': 'system', 'content': 'Please answer with short sentence.'}]\n",
    "    for x, y in zip(SystemMsgs, UserMsgs):\n",
    "        msg.append({'role': 'assistant', 'content': x})\n",
    "        msg.append({'role': 'user', 'content': y})\n",
    "    llm = hf.chat.completions.create(\n",
    "        model= 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        messages=msg,\n",
    "        tools=tools\n",
    "    )\n",
    "    print('tools',llm.choices[0])\n",
    "    if llm.choices[0].finish_reason=='stop':\n",
    "        return 'omg!'\n",
    "    else:\n",
    "        res = llm.choices[0].message.content\n",
    "        SystemMsgs.append(res)\n",
    "        return res\n",
    "ticket_prices = {'la': '$2000', 'paris': '$899', 'tokyo': '$1400', 'berlin': '$499'}\n",
    "def get_ticket_price(city):\n",
    "    print(f'Tool get_ticket_price called for {city}')\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, 'Unknown')\n",
    "\n",
    "\n",
    "# print(f\"A: How much to go to LA?\")\n",
    "# b_next = saySystem('How much to go to LA?')\n",
    "print(f\"A: Hey, what's the weather like in Paris right now?\")\n",
    "b_next = saySystem('How are you today?')\n",
    "print(f\"B: {b_next}\")\n",
    "# b_next = saySystem('Can you tell me more about it?')\n",
    "# print(f\"A: Thank you for the answer.\")\n",
    "# print(f\"B: {b_next}\")\n",
    "# b_next = saySystem('I see. Thank you!')\n",
    "# print(f\"A: I see. Thank you!\")\n",
    "# print(f\"B: {b_next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71452c36-e544-4d84-8a31-5816580ec220",
   "metadata": {},
   "source": [
    "# 3 Generate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1093411-0a0a-48ab-99b6-21929d1fab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "import subprocess\n",
    "load_dotenv()\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "hf = InferenceClient('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "def sys_settting():\n",
    "    x = 'You are an assistant that reimplements Python code in high performance C++ for an M3 Mac. '\n",
    "    x += 'Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. '\n",
    "    x += 'The C++ response needs to produce an identical output in the fastest possible time.'\n",
    "    return x\n",
    "def usr_settting_and_code(x1):\n",
    "    x2 = 'Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. '\n",
    "    x2 += 'Respond only with C++ code; do not explain your work other than a few comments. '\n",
    "    x2 += 'Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n'\n",
    "    x2 += x1\n",
    "    return x2\n",
    "def sys_usr_setting_and_code(x):\n",
    "    return [\n",
    "        {'role': 'system', 'content': sys_settting()},\n",
    "        {'role': 'user', 'content': usr_settting_and_code(x)}\n",
    "    ]\n",
    "def write_output(cpp):\n",
    "    code = cpp.replace('```cpp','').replace('```','')\n",
    "    with open('code.cpp', 'w') as f:\n",
    "        f.write(code)\n",
    "def py_to_cpp_hf(x):        \n",
    "    llm = hf.chat.completions.create(\n",
    "        model= 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        messages=sys_usr_setting_and_code(x)\n",
    "    )\n",
    "    write_output(llm.choices[0].message.content)\n",
    "    return llm.choices[0].message.content\n",
    "def run_cpp():\n",
    "        try:\n",
    "            run1 = subprocess.run(['clang++', '-Ofast', '-std=c++17', '-march=armv8.5-a', '-mtune=apple-m1', '-mcpu=apple-m1', '-o', 'code', 'code.cpp'], check=True, text=True, capture_output=True)\n",
    "            run2 = subprocess.run(['./code'], check=True, text=True, capture_output=True)\n",
    "            return run2.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return f'An error occurred:\\n{e.stderr}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd3b06e-6f10-4b87-b07b-13132540b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "1. python code:\n",
      " \n",
      "import time\n",
      "def echo():\n",
      "    return 'apple'\n",
      "start_time = time.time()\n",
      "result = echo()\n",
      "end_time = time.time()\n",
      "print(\"Result:\", result)\n",
      "print(\"Execution Time: {:.6f}s\".format(end_time-start_time))\n",
      "\n",
      "------------------\n",
      "2. run:\n",
      "Result: apple\n",
      "Execution Time: 0.000001s\n",
      "None\n",
      "------------------\n",
      "3. c++ code \n",
      " ```cpp\n",
      "#include <iostream>\n",
      "#include <iomanip>\n",
      "#include <chrono>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "string echo() {\n",
      "    return \"apple\";\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    auto start_time = chrono::high_resolution_clock::now();\n",
      "    string result = echo();\n",
      "    auto end_time = chrono::high_resolution_clock::now();\n",
      "\n",
      "    cout << \"Result: \" << result << endl;\n",
      "    auto duration = chrono::duration_cast<chrono::duration<double>>(end_time - start_time);\n",
      "    cout << \"Execution Time: \" << setprecision(6) << duration.count() << \"s\" << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "4. run:\n",
      "Result: apple\n",
      "Execution Time: 4.1e-08s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code1 = '''\n",
    "print('hi')\n",
    "'''\n",
    "code2 = '''\n",
    "import time\n",
    "def echo():\n",
    "    return 'apple'\n",
    "start_time = time.time()\n",
    "result = echo()\n",
    "end_time = time.time()\n",
    "print(\"Result:\", result)\n",
    "print(\"Execution Time: {:.6f}s\".format(end_time-start_time))\n",
    "'''\n",
    "# python show and run code\n",
    "print('------------------')\n",
    "print('1. python code:\\n', code2)\n",
    "print('------------------')\n",
    "print('2. run:')\n",
    "print(exec(code2))\n",
    "# cpp show and run code(hf)\n",
    "print('------------------')\n",
    "print('3. c++ code \\n',py_to_cpp_hf(code2))\n",
    "# print('------------------')\n",
    "print('4. run:')\n",
    "print(run_cpp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300c792b-2634-4a52-a6e1-b73f4642b4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "1. python code:\n",
      " \n",
      "import time\n",
      "\n",
      "def calculate(iterations, param1, param2):\n",
      "    result = 1.0\n",
      "    for i in range(1, iterations+1):\n",
      "        j = i * param1 - param2\n",
      "        result -= (1/j)\n",
      "        j = i * param1 + param2\n",
      "        result += (1/j)\n",
      "    return result\n",
      "\n",
      "start_time = time.time()\n",
      "result = calculate(100_000_000, 4, 1) * 4\n",
      "end_time = time.time()\n",
      "\n",
      "print(f\"Result: {result:.12f}\")\n",
      "print(f\"Execution Time: {(end_time-start_time):.6f} seconds\")\n",
      "\n",
      "------------------\n",
      "2. run:\n",
      "Result: 3.141592658589\n",
      "Execution Time: 5.770831 seconds\n",
      "None\n",
      "------------------\n",
      "3. c++ code \n",
      " ```cpp\n",
      "#include <iostream>\n",
      "#include <iomanip>\n",
      "#include <chrono>\n",
      "\n",
      "double calculate(int iterations, double param1, double param2) {\n",
      "    double result = 1.0;\n",
      "    for (int i = 1; i <= iterations; ++i) {\n",
      "        double j;\n",
      "        j = i * param1 - param2;\n",
      "        result -= 1.0 / j;\n",
      "        j = i * param1 + param2;\n",
      "        result += 1.0 / j;\n",
      "    }\n",
      "    return result * 4;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    auto start_time = std::chrono::high_resolution_clock::now();\n",
      "    double result = calculate(100_000_000, 4, 1);\n",
      "    auto end_time = std::chrono::high_resolution_clock::now();\n",
      "    std::cout << \"Result: \" << std::setprecision(12) << result << std::endl;\n",
      "    std::cout << \"Execution Time: \" << std::setprecision(6) << std::chrono::duration_cast<std::chrono::duration<double>>(end_time - start_time).count() << \" seconds\" << std::endl;\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "------------------\n",
      "4. run:\n",
      "An error occurred:\n",
      "code.cpp:20:34: error: no matching literal operator for call to 'operator\"\"_000_000' with argument of type 'unsigned long long' or 'const char *', and no matching literal operator template\n",
      "    double result = calculate(100_000_000, 4, 1);\n",
      "                                 ^\n",
      "1 error generated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time-start_time):.6f} seconds\")\n",
    "\"\"\"\n",
    "# python show and run code\n",
    "print('------------------')\n",
    "print('1. python code:\\n', pi)\n",
    "print('------------------')\n",
    "print('2. run:')\n",
    "print(exec(pi))\n",
    "# cpp show and run code(hf)\n",
    "print('------------------')\n",
    "print('3. c++ code \\n',py_to_cpp_hf(pi))\n",
    "print('------------------')\n",
    "print('4. run:')\n",
    "print(run_cpp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea43ea0-5ce7-4763-8d68-3464485919e7",
   "metadata": {},
   "source": [
    "# 4 Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a34e067f-2187-42de-93a4-a9bbc34c7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hola.'}]\n"
     ]
    }
   ],
   "source": [
    "# TRANSLATION\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "# English to Spanish\n",
    "API_URL = 'https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-es'\n",
    "# English to French\n",
    "# API_URL = 'https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-fr'\n",
    "# English to German\n",
    "# API_URL = 'https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-en-de'\n",
    "# Helsinki-NLP/opus-tatoeba-en-ja\n",
    "# English to Japanese\n",
    "# API_URL = 'https://api-inference.huggingface.co/models/Helsinki-NLP/opus-tatoeba-en-ja'\n",
    "# Japanese to English\n",
    "# API_URL = 'https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ja-en'\n",
    "\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    time.sleep(1)\n",
    "    while True:\n",
    "      try:\n",
    "        response = requests.request('POST', API_URL, headers=headers, data=data)\n",
    "        break\n",
    "      except Exception:\n",
    "          continue\n",
    "    return json.loads(response.content.decode('utf-8'))\n",
    "print(query({'inputs': 'Hello'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ba05c-3f73-462e-adb5-feccdd69e24b",
   "metadata": {},
   "source": [
    "# 5 measure customer temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2303e733-0e28-4e01-91e2-4048f4066291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Katie24546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfFolder, whoami, create_inference_endpoint, login, InferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "HfFolder.save_token(hf_token)\n",
    "user = whoami()\n",
    "print(f\"Logged in as {user['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df3b303-d2e8-4475-90e7-09a37bc0b52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9996525049209595}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "res= classifier(\"I like macdonald\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189ca2ab-8129-4d63-bdce-b3ae6cab4fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9996525049209595}]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier=pipeline('sentiment-analysis',model=model,tokenizer=tokenizer)\n",
    "res=classifier(\"I like macdonald\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43461fa9-73bc-4814-ba8c-5fdb81640a82",
   "metadata": {},
   "source": [
    "# ? Open Question with sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3450e-bf29-479a-8ac5-a487552b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hf_token = os.environ['HF_TOKEN']\n",
    "headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "API_URL = 'https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B'\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "print((query({'inputs': 'Which city is the capital of United States? ',}))[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64c70b-9c13-4ee8-8c5a-5567efd2d379",
   "metadata": {},
   "source": [
    "# Login for set endpoint and get stream answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a47d31-66b6-4342-8470-ec43ba572128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722]\n",
      "using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "sequence = 'Using a Transformer network is simple'\n",
    "res = tokenizer(sequence)\n",
    "print(res)\n",
    "tokens=tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8fbeb-56a2-4360-9dab-cb70919a653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(task='conversational, model='./models/facebook/blenderbot-400M-distill')\n",
    "user_message = 'Which are some best places to visit during summer?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "664c2a76-d6e9-440f-a5e6-98d8e384f04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "text1 = 'Whirlpool Igniter This is a Genuine OEM Replacement Part. Manufacturer Whirlpool, Part Weight 1 pounds, Dimensions 1 x 1 x 1 inches, model number Quantity 1, Warranty Description This is a Genuine OEM Replacement Part., Rank Tools & Home Improvement Parts & Accessories 93370, Available April 19, 2019'\n",
    "\n",
    "encode1=tokenizer(text1, padding=True, add_special_tokens=True)\n",
    "encode2 = tokenizer.convert_ids_to_tokens(encode1.input_ids)\n",
    "print(len(encode2))\n",
    "# tokenizer.convert_ids_to_tokens([0,100,101,102,103])\n",
    "\n",
    "# text2 = tokenizer.decode(encode)\n",
    "# print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e90dc6-f560-4ff0-ac3f-2226535bae03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
